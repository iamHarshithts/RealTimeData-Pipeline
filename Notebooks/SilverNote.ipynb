{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70cd3fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 19:52:05 WARN Utils: Your hostname, harshithts-HP-Pavilion-Gaming-Laptop-15-ec2xxx resolves to a loopback address: 127.0.1.1; using 192.168.1.4 instead (on interface wlo1)\n",
      "25/11/26 19:52:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/harshithts/RealTimeData-Pipeline/env/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/harshithts/.ivy2/cache\n",
      "The jars for the packages stored in: /home/harshithts/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-33831197-779c-4d2e-bfd3-0b6c735dcfa0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 404ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-33831197-779c-4d2e-bfd3-0b6c735dcfa0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "25/11/26 19:52:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/26 19:52:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#Setting Spark with MinIO\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,FloatType\n",
    "from pyspark.sql.functions import col,to_date,date_format,split,split,trim,upper,regexp_replace\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "S3_ACCESS_KEY = os.getenv(\"S3_ACCESS_KEY\")\n",
    "S3_SECRET_KEY = os.getenv(\"S3_SECRET_KEY\")\n",
    "S3_ENDPOINT = os.getenv(\"S3_ENDPOINT\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIOReader\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "spark.catalog.clearCache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5530bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema defining\n",
    "Profile_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"account_type\", StringType(), True),\n",
    "    StructField(\"credit_score\", IntegerType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "merchant_schema = StructType([\n",
    "    StructField(\"merchant_id\", StringType(), True),\n",
    "    StructField(\"merchant_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"risk_level\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "account_schema = StructType([\n",
    "    StructField(\"account_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"current_balance\", FloatType(), True),\n",
    "    StructField(\"currency\", StringType(), True),\n",
    "    StructField(\"last_updated\", StringType(), True),\n",
    "  \n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e7b40aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 19:52:20 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#reading data from the buckets\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "profile_df=spark.read.format(\"json\")\\\n",
    "    .schema(Profile_schema)\\\n",
    "    .load(f\"s3a://{BUCKET_NAME}/bronze/profiles-folder/*.json\")\n",
    "\n",
    "merchant_df= spark.read.format(\"json\")\\\n",
    "    .schema(merchant_schema)\\\n",
    "    .load(f\"s3a://{BUCKET_NAME}/bronze/merchant-folder/*.json\")\n",
    "\n",
    "account_df= spark.read.format(\"json\")\\\n",
    "    .schema(account_schema)\\\n",
    "    .load(f\"s3a://{BUCKET_NAME}/bronze/account-folder/*.json\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2747bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Number of partitions: 2\n",
      "Number of partitions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§© Partition 0: 5 sample rows\n",
      "Row(user_id='U0008', name='Patrick Morrison', country='Palestinian Territory', account_type='Current', credit_score=590, status='suspended', timestamp='2025-01-12T01:34:43.903322')\n",
      "Row(user_id='U0010', name='Tommy Ramos', country='United Arab Emirates', account_type='Savings', credit_score=507, status='active', timestamp='2025-03-03T20:34:28.576867')\n",
      "Row(user_id='U0007', name='Timothy Weber', country='Uzbekistan', account_type='Current', credit_score=471, status='suspended', timestamp='2025-04-05T05:29:57.884762')\n",
      "Row(user_id='U0006', name='Edward Reyes', country='Iceland', account_type='Savings', credit_score=713, status='suspended', timestamp='2025-10-16T00:03:28.351299')\n",
      "Row(user_id='U0009', name='Benjamin Rhodes', country='Korea', account_type='Current', credit_score=389, status='closed', timestamp='2025-06-11T18:47:59.473427')\n",
      "\n",
      "ðŸ§© Partition 1: 5 sample rows\n",
      "Row(user_id='U0001', name='Holly Rodriguez', country='Peru', account_type='Current', credit_score=519, status='closed', timestamp='2025-01-31T09:10:11.386181')\n",
      "Row(user_id='U0002', name='Jason Gregory', country='Guyana', account_type='Current', credit_score=838, status='active', timestamp='2025-10-13T09:08:31.829745')\n",
      "Row(user_id='U0005', name='Marie Wilson', country='Sudan', account_type='Savings', credit_score=522, status='closed', timestamp='2025-09-30T06:04:08.783345')\n",
      "Row(user_id='U0003', name='Amy Roy', country='Hong Kong', account_type='Savings', credit_score=384, status='closed', timestamp='2025-10-20T07:24:26.692550')\n",
      "Row(user_id='U0004', name='Chris Wilson', country='Oman', account_type='Savings', credit_score=568, status='active', timestamp='2025-03-12T12:34:48.341365')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#coalescing the dataframes to 2 partitions each\n",
    "profile_df = profile_df.coalesce(2)\n",
    "merchant_df = merchant_df.coalesce(2)\n",
    "account_df = account_df.coalesce(2)\n",
    "\n",
    "print(\"Number of partitions:\", profile_df.rdd.getNumPartitions())\n",
    "print(\"Number of partitions:\", merchant_df.rdd.getNumPartitions())\n",
    "print(\"Number of partitions:\", account_df.rdd.getNumPartitions())\n",
    "\n",
    "partition_data = profile_df.rdd.mapPartitionsWithIndex(\n",
    "    lambda idx, it: [(idx, list(it))] \n",
    ").collect()\n",
    "\n",
    "for idx, rows in partition_data:\n",
    "    print(f\"\\nðŸ§© Partition {idx}: {len(rows)} sample rows\")\n",
    "    for r in rows:\n",
    "        print(r)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c50dc7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+--------------------+------------+------------+---------+----------+---------+----+--------+\n",
      "|user_id|first_name|last_name|             country|account_type|credit_score|   status|      date|    month|year|    time|\n",
      "+-------+----------+---------+--------------------+------------+------------+---------+----------+---------+----+--------+\n",
      "|  U0001|     Holly|Rodriguez|                Peru|     Current|         519|   closed|2025-01-31|  January|2025|09:10:11|\n",
      "|  U0002|     Jason|  Gregory|              Guyana|     Current|         838|   active|2025-10-13|  October|2025|09:08:31|\n",
      "|  U0003|       Amy|      Roy|            HongKong|     Savings|         384|   closed|2025-10-20|  October|2025|07:24:26|\n",
      "|  U0004|     Chris|   Wilson|                Oman|     Savings|         568|   active|2025-03-12|    March|2025|12:34:48|\n",
      "|  U0005|     Marie|   Wilson|               Sudan|     Savings|         522|   closed|2025-09-30|September|2025|06:04:08|\n",
      "|  U0006|    Edward|    Reyes|             Iceland|     Savings|         713|suspended|2025-10-16|  October|2025|00:03:28|\n",
      "|  U0007|   Timothy|    Weber|          Uzbekistan|     Current|         471|suspended|2025-04-05|    April|2025|05:29:57|\n",
      "|  U0008|   Patrick| Morrison|PalestinianTerritory|     Current|         590|suspended|2025-01-12|  January|2025|01:34:43|\n",
      "|  U0009|  Benjamin|   Rhodes|               Korea|     Current|         389|   closed|2025-06-11|     June|2025|18:47:59|\n",
      "|  U0010|     Tommy|    Ramos|  UnitedArabEmirates|     Savings|         507|   active|2025-03-03|    March|2025|20:34:28|\n",
      "+-------+----------+---------+--------------------+------------+------------+---------+----------+---------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#esnure every user_id starts with U \n",
    "profile_df = profile_df.filter(col(\"user_id\").startswith(\"U\"))\n",
    "\n",
    "#transforming the timestamp column to date, month and year for profile dataframe\n",
    "\n",
    "profile_df = profile_df.withColumn(\"date\", to_date(col(\"timestamp\"), \"yyyy-MM-dd\"))\\\n",
    ".withColumn(\"month\", date_format(\"date\", \"MMMM\"))\\\n",
    ".withColumn(\"year\", date_format(\"date\", \"yyyy\"))\\\n",
    ".withColumn(\"time\",date_format(col(\"timestamp\"), \"HH:mm:ss\"))\\\n",
    ".withColumn(\"country\",regexp_replace(trim(col(\"country\")), \" \", \"\"))\n",
    "\n",
    "\n",
    "profile_df = profile_df\n",
    "\n",
    "\n",
    "#splitted names to first and last names\n",
    "profile_df = profile_df.withColumn(\"first_name\", split(col(\"name\"), \" \").getItem(0))\n",
    "profile_df = profile_df.withColumn(\"last_name\", split(col(\"name\"), \" \",).getItem(1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#handling duplicates\n",
    "\n",
    "profile_duplictes = profile_df.groupBy(\"user_id\")\\\n",
    "            .count()\\\n",
    "            .filter(\"count > 1\")\n",
    "\n",
    "profile_df = profile_df.dropDuplicates([\"user_id\"])\n",
    "\n",
    "profile_df = profile_df.select('user_id', 'first_name', 'last_name', 'country', 'account_type', 'credit_score', 'status', 'date', 'month', 'year','time')\n",
    "profile_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ff0bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+-------+----------+----------+--------+----+\n",
      "|merchant_id|       merchant_name|  category|country|risk_level|      date|   month|year|\n",
      "+-----------+--------------------+----------+-------+----------+----------+--------+----+\n",
      "|      M0001|        Collins-Love|Restaurant|    USA|      High|2025-05-21|     May|2025|\n",
      "|      M0002|      Robinson-Hogan|    Retail|    USA|      High|2025-08-14|  August|2025|\n",
      "|      M0003|         Rojas-Smith|    Retail| Russia|    Medium|2025-10-01| October|2025|\n",
      "|      M0004|       Gamble-Weaver|      Fuel| Russia|      High|2025-05-08|     May|2025|\n",
      "|      M0005|    Middleton-Morton|    Retail|    USA|      High|2025-03-21|   March|2025|\n",
      "|      M0006|   Singleton-Jenkins|    Retail|  India|       Low|2025-02-10|February|2025|\n",
      "|      M0007|      Mcintosh-Burns|    Travel|  India|      High|2025-08-12|  August|2025|\n",
      "|      M0008|Arnold, Harper an...|      Fuel| Russia|    Medium|2025-02-02|February|2025|\n",
      "|      M0009|Hill, Rodriguez a...|      Fuel|  India|      High|2025-10-09| October|2025|\n",
      "|      M0010|         Shelton Ltd|E-commerce|  India|       Low|2025-11-17|November|2025|\n",
      "+-----------+--------------------+----------+-------+----------+----------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ensurinq that merchant id starts with M\n",
    "merchant_df = merchant_df.filter(col(\"merchant_id\").startswith(\"M\"))\n",
    "\n",
    "#duplicate records \n",
    "duplicates_key_df = (\n",
    "    merchant_df.groupBy(\"merchant_id\")\n",
    "    .count()\n",
    "    .filter(\"count > 1\")\n",
    "    .select(\"merchant_id\")\n",
    ")\n",
    "\n",
    "#removing the dupicates\n",
    "merchant_df = merchant_df.dropDuplicates([\"merchant_id\"])\n",
    "\n",
    "#trimming all string data columns\n",
    "\n",
    "def trim_all_string_columns(merchant_df):\n",
    "\n",
    "    string_cols = [f.name for f in merchant_df.schema.fields if f.dataType.simpleString() == \"string\"]\n",
    "    for c in string_cols:\n",
    "        merchant_df = merchant_df.withColumn(c, trim(col(c)))\n",
    "    return merchant_df\n",
    "\n",
    "merchant_df = trim_all_string_columns(merchant_df)\n",
    "\n",
    "#tranfroming the timestamp column to date month and year for merchant data\n",
    "\n",
    "merchant_df = merchant_df.withColumn(\"date\", to_date(col(\"timestamp\"), \"yyyy-MM-dd\"))\n",
    "merchant_df = merchant_df.withColumn(\"month\", date_format(\"date\", \"MMMM\"))\n",
    "merchant_df = merchant_df.withColumn(\"year\", date_format(\"date\", \"yyyy\"))\n",
    "merchant_df = merchant_df.select('merchant_id', 'merchant_name', 'category', 'country', 'risk_level', 'date', 'month', 'year')\n",
    "\n",
    "\n",
    "merchant_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14420c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------------+--------+----------+---------+----+--------+--------------------+\n",
      "|account_id|user_id|current_balance|currency|      date|    month|year|    time|        last_updated|\n",
      "+----------+-------+---------------+--------+----------+---------+----+--------+--------------------+\n",
      "|     A0001|  U0001|       29800.61|     INR|2025-03-08|    March|2025|13:52:48|2025-03-08T13:52:...|\n",
      "|     A0002|  U0002|       94011.69|     INR|2025-07-09|     July|2025|22:34:02|2025-07-09T22:34:...|\n",
      "|     A0003|  U0003|      130480.28|     INR|2025-07-10|     July|2025|07:15:24|2025-07-10T07:15:...|\n",
      "|     A0004|  U0004|      172161.48|     INR|2025-05-31|      May|2025|05:37:11|2025-05-31T05:37:...|\n",
      "|     A0005|  U0005|      138958.44|     INR|2025-03-14|    March|2025|17:27:39|2025-03-14T17:27:...|\n",
      "|     A0006|  U0006|       84217.01|     INR|2025-11-17| November|2025|08:10:07|2025-11-17T08:10:...|\n",
      "|     A0007|  U0007|      132478.67|     INR|2025-05-12|      May|2025|04:24:57|2025-05-12T04:24:...|\n",
      "|     A0008|  U0008|       67038.53|     INR|2025-08-02|   August|2025|16:49:32|2025-08-02T16:49:...|\n",
      "|     A0009|  U0009|      172444.98|     INR|2025-04-04|    April|2025|15:54:57|2025-04-04T15:54:...|\n",
      "|     A0010|  U0010|        60012.3|     INR|2025-09-21|September|2025|01:46:03|2025-09-21T01:46:...|\n",
      "+----------+-------+---------------+--------+----------+---------+----+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#account id starts with A and user id starts with U\n",
    "account_df = account_df.filter(col(\"account_id\").startswith(\"A\") & col(\"user_id\").startswith(\"U\")) \n",
    "                       \n",
    "#extracting date month year from last_updated column\n",
    "account_df = account_df.withColumn(\"date\", to_date(col(\"last_updated\"), \"yyyy-MM-dd\"))\\\n",
    "                    .withColumn(\"month\", date_format(\"date\", \"MMMM\")) \\\n",
    "                    .withColumn(\"year\", date_format(\"date\", \"yyyy\"))\\\n",
    "                    .withColumn(\"Time\",date_format(col(\"last_updated\"), \"HH:mm:ss\"))\n",
    "\n",
    "# #ensuring the userid , accountid , cuurency are Uppercase\n",
    "\n",
    "account_df = account_df.withColumn(\"account_id\", upper(col(\"account_id\"))) \\\n",
    "                       .withColumn(\"user_id\", upper(col(\"user_id\"))) \\\n",
    "                       .withColumn(\"currency\", upper(col(\"currency\")))\n",
    "##duplicate datas\n",
    "duplicate_keys = (\n",
    "    account_df.groupBy(\"account_id\")\n",
    "    .count()\n",
    "    .filter(\"count > 1\")\n",
    ")\n",
    "account_df = account_df.dropDuplicates([\"account_id\"])\n",
    "\n",
    "#filtering the account data for current balance greater than 0 and not null currency\n",
    "\n",
    "account_df = account_df.filter((col(\"current_balance\") > 0) & (col(\"currency\").isNotNull()))\n",
    "\n",
    "account_df.select('account_id', 'user_id', 'current_balance', 'currency', 'date', 'month', 'year','time','last_updated').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d54b1892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "profile_df.write.format(\"parquet\").mode(\"append\").save(f\"s3a://{BUCKET_NAME}/silver/processed-profiles/\")\n",
    "merchant_df.write.format(\"parquet\").mode(\"append\").save(f\"s3a://{BUCKET_NAME}/silver/processed-merchants/\")\n",
    "account_df.write.format(\"parquet\").mode(\"append\").save(f\"s3a://{BUCKET_NAME}/silver/processed-accounts/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
