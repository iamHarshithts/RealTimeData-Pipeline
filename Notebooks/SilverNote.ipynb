{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70cd3fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/15 16:51:40 WARN Utils: Your hostname, harshithts-HP-Pavilion-Gaming-Laptop-15-ec2xxx resolves to a loopback address: 127.0.1.1; using 192.168.1.2 instead (on interface wlo1)\n",
      "25/11/15 16:51:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/harshithts/Documents/sturctruing/env/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/harshithts/.ivy2/cache\n",
      "The jars for the packages stored in: /home/harshithts/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-17e99a46-f492-4abe-84da-5a43fa51894f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 186ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-17e99a46-f492-4abe-84da-5a43fa51894f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "25/11/15 16:51:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#Setting Spark with MinIO\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,FloatType\n",
    "from pyspark.sql.functions import col,to_date,date_format,split,split,trim,upper,regexp_replace\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "S3_ACCESS_KEY = os.getenv(\"S3_ACCESS_KEY\")\n",
    "S3_SECRET_KEY = os.getenv(\"S3_SECRET_KEY\")\n",
    "S3_ENDPOINT = os.getenv(\"S3_ENDPOINT\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIOReader\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "spark.catalog.clearCache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5530bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema defining\n",
    "Profile_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"account_type\", StringType(), True),\n",
    "    StructField(\"credit_score\", IntegerType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "merchant_schema = StructType([\n",
    "    StructField(\"merchant_id\", StringType(), True),\n",
    "    StructField(\"merchant_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"risk_level\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "account_schema = StructType([\n",
    "    StructField(\"account_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"current_balance\", FloatType(), True),\n",
    "    StructField(\"currency\", StringType(), True),\n",
    "    StructField(\"last_updated\", StringType(), True),\n",
    "  \n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e7b40aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/15 16:51:48 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#reading data from the buckets\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "profile_df=spark.read.format(\"json\")\\\n",
    "    .schema(Profile_schema)\\\n",
    "    .load(f\"s3a://{BUCKET_NAME}/bronze/profiles-folder/*.json\")\n",
    "\n",
    "merchant_df= spark.read.format(\"json\")\\\n",
    "    .schema(merchant_schema)\\\n",
    "    .load(f\"s3a://{BUCKET_NAME}/bronze/merchant-folder/*.json\")\n",
    "\n",
    "account_df= spark.read.format(\"json\")\\\n",
    "    .schema(account_schema)\\\n",
    "    .load(f\"s3a://{BUCKET_NAME}/bronze/account-folder/*.json\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2747bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Number of partitions: 2\n",
      "Number of partitions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§© Partition 0: 5 sample rows\n",
      "Row(user_id='U0003', name='Michael Wright', country='Syrian Arab Republic', account_type='Savings', credit_score=766, status='suspended', timestamp='2025-03-09T01:55:51.245730')\n",
      "Row(user_id='U0006', name='Kelsey Garcia', country='Norfolk Island', account_type='Savings', credit_score=576, status='closed', timestamp='2025-08-05T03:56:01.088296')\n",
      "Row(user_id='U0001', name='Melissa Alexander', country='Mongolia', account_type='Savings', credit_score=771, status='closed', timestamp='2025-09-17T11:34:59.637391')\n",
      "Row(user_id='U0004', name='Colin Rivera', country='Mauritania', account_type='Current', credit_score=776, status='suspended', timestamp='2025-05-02T05:29:44.735715')\n",
      "Row(user_id='U0010', name='Michael Murray', country='Comoros', account_type='Current', credit_score=767, status='suspended', timestamp='2025-11-03T10:48:17.294047')\n",
      "\n",
      "ðŸ§© Partition 1: 5 sample rows\n",
      "Row(user_id='U0005', name='Elizabeth Pierce', country='Nigeria', account_type='Savings', credit_score=845, status='active', timestamp='2025-07-28T16:50:39.666995')\n",
      "Row(user_id='U0007', name='Mrs. Amanda Carroll', country='Cuba', account_type='Current', credit_score=635, status='closed', timestamp='2025-03-24T04:13:55.119400')\n",
      "Row(user_id='U0008', name='Mary Williams', country='Senegal', account_type='Savings', credit_score=754, status='suspended', timestamp='2025-09-26T21:40:12.309361')\n",
      "Row(user_id='U0002', name='Lindsey Wallace', country='Denmark', account_type='Savings', credit_score=566, status='closed', timestamp='2025-01-01T00:00:53.898914')\n",
      "Row(user_id='U0009', name='Tracy Vasquez', country='Gibraltar', account_type='Current', credit_score=596, status='active', timestamp='2025-01-21T08:50:58.025007')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#coalescing the dataframes to 2 partitions each\n",
    "profile_df = profile_df.coalesce(2)\n",
    "merchant_df = merchant_df.coalesce(2)\n",
    "account_df = account_df.coalesce(2)\n",
    "\n",
    "print(\"Number of partitions:\", profile_df.rdd.getNumPartitions())\n",
    "print(\"Number of partitions:\", merchant_df.rdd.getNumPartitions())\n",
    "print(\"Number of partitions:\", account_df.rdd.getNumPartitions())\n",
    "\n",
    "partition_data = profile_df.rdd.mapPartitionsWithIndex(\n",
    "    lambda idx, it: [(idx, list(it))] \n",
    ").collect()\n",
    "\n",
    "for idx, rows in partition_data:\n",
    "    print(f\"\\nðŸ§© Partition {idx}: {len(rows)} sample rows\")\n",
    "    for r in rows:\n",
    "        print(r)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c50dc7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------------------+------------+------------+---------+----------+---------+----+--------+\n",
      "|user_id|first_name|last_name|           country|account_type|credit_score|   status|      date|    month|year|    time|\n",
      "+-------+----------+---------+------------------+------------+------------+---------+----------+---------+----+--------+\n",
      "|  U0001|   Melissa|Alexander|          Mongolia|     Savings|         771|   closed|2025-09-17|September|2025|11:34:59|\n",
      "|  U0002|   Lindsey|  Wallace|           Denmark|     Savings|         566|   closed|2025-01-01|  January|2025|00:00:53|\n",
      "|  U0003|   Michael|   Wright|SyrianArabRepublic|     Savings|         766|suspended|2025-03-09|    March|2025|01:55:51|\n",
      "|  U0004|     Colin|   Rivera|        Mauritania|     Current|         776|suspended|2025-05-02|      May|2025|05:29:44|\n",
      "|  U0005| Elizabeth|   Pierce|           Nigeria|     Savings|         845|   active|2025-07-28|     July|2025|16:50:39|\n",
      "|  U0006|    Kelsey|   Garcia|     NorfolkIsland|     Savings|         576|   closed|2025-08-05|   August|2025|03:56:01|\n",
      "|  U0007|      Mrs.|   Amanda|              Cuba|     Current|         635|   closed|2025-03-24|    March|2025|04:13:55|\n",
      "|  U0008|      Mary| Williams|           Senegal|     Savings|         754|suspended|2025-09-26|September|2025|21:40:12|\n",
      "|  U0009|     Tracy|  Vasquez|         Gibraltar|     Current|         596|   active|2025-01-21|  January|2025|08:50:58|\n",
      "|  U0010|   Michael|   Murray|           Comoros|     Current|         767|suspended|2025-11-03| November|2025|10:48:17|\n",
      "+-------+----------+---------+------------------+------------+------------+---------+----------+---------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#esnure every user_id starts with U \n",
    "profile_df = profile_df.filter(col(\"user_id\").startswith(\"U\"))\n",
    "\n",
    "#transforming the timestamp column to date, month and year for profile dataframe\n",
    "\n",
    "profile_df = profile_df.withColumn(\"date\", to_date(col(\"timestamp\"), \"yyyy-MM-dd\"))\\\n",
    ".withColumn(\"month\", date_format(\"date\", \"MMMM\"))\\\n",
    ".withColumn(\"year\", date_format(\"date\", \"yyyy\"))\\\n",
    ".withColumn(\"time\",date_format(col(\"timestamp\"), \"HH:mm:ss\"))\\\n",
    ".withColumn(\"country\",regexp_replace(trim(col(\"country\")), \" \", \"\"))\n",
    "\n",
    "\n",
    "profile_df = profile_df\n",
    "\n",
    "\n",
    "#splitted names to first and last names\n",
    "profile_df = profile_df.withColumn(\"first_name\", split(col(\"name\"), \" \").getItem(0))\n",
    "profile_df = profile_df.withColumn(\"last_name\", split(col(\"name\"), \" \",).getItem(1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#handling duplicates\n",
    "\n",
    "profile_duplictes = profile_df.groupBy(\"user_id\")\\\n",
    "            .count()\\\n",
    "            .filter(\"count > 1\")\n",
    "\n",
    "profile_df = profile_df.dropDuplicates([\"user_id\"])\n",
    "\n",
    "profile_df = profile_df.select('user_id', 'first_name', 'last_name', 'country', 'account_type', 'credit_score', 'status', 'date', 'month', 'year','time')\n",
    "profile_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ff0bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+-------+----------+----------+---------+----+\n",
      "|merchant_id|       merchant_name|  category|country|risk_level|      date|    month|year|\n",
      "+-----------+--------------------+----------+-------+----------+----------+---------+----+\n",
      "|      M0001|Hill, Gonzales an...|    Travel|  India|    Medium|2025-01-01|  January|2025|\n",
      "|      M0002|      Martinez-Owens|    Retail|  India|       Low|2025-04-24|    April|2025|\n",
      "|      M0003|Waller, Mueller a...|Restaurant| Russia|      High|2025-03-31|    March|2025|\n",
      "|      M0004|     Oconnor-Carroll|Restaurant|    USA|       Low|2025-09-26|September|2025|\n",
      "|      M0005|       Bennett-Moore|    Travel|  India|       Low|2025-10-25|  October|2025|\n",
      "|      M0006|         Torres-Long|    Retail|  India|       Low|2025-06-24|     June|2025|\n",
      "|      M0007|        Jones-Chavez|    Travel| Russia|    Medium|2025-01-02|  January|2025|\n",
      "|      M0008|       Harris-Murphy|Restaurant| Russia|      High|2025-02-23| February|2025|\n",
      "|      M0009|Hayes, Williams a...|E-commerce|    USA|    Medium|2025-02-27| February|2025|\n",
      "|      M0010|       Alvarez Group|Restaurant|  India|      High|2025-06-04|     June|2025|\n",
      "+-----------+--------------------+----------+-------+----------+----------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ensurinq that merchant id starts with M\n",
    "merchant_df = merchant_df.filter(col(\"merchant_id\").startswith(\"M\"))\n",
    "\n",
    "#duplicate records \n",
    "duplicates_key_df = (\n",
    "    merchant_df.groupBy(\"merchant_id\")\n",
    "    .count()\n",
    "    .filter(\"count > 1\")\n",
    "    .select(\"merchant_id\")\n",
    ")\n",
    "\n",
    "#removing the dupicates\n",
    "merchant_df = merchant_df.dropDuplicates([\"merchant_id\"])\n",
    "\n",
    "#trimming all string data columns\n",
    "\n",
    "def trim_all_string_columns(merchant_df):\n",
    "\n",
    "    string_cols = [f.name for f in merchant_df.schema.fields if f.dataType.simpleString() == \"string\"]\n",
    "    for c in string_cols:\n",
    "        merchant_df = merchant_df.withColumn(c, trim(col(c)))\n",
    "    return merchant_df\n",
    "\n",
    "merchant_df = trim_all_string_columns(merchant_df)\n",
    "\n",
    "#tranfroming the timestamp column to date month and year for merchant data\n",
    "\n",
    "merchant_df = merchant_df.withColumn(\"date\", to_date(col(\"timestamp\"), \"yyyy-MM-dd\"))\n",
    "merchant_df = merchant_df.withColumn(\"month\", date_format(\"date\", \"MMMM\"))\n",
    "merchant_df = merchant_df.withColumn(\"year\", date_format(\"date\", \"yyyy\"))\n",
    "merchant_df = merchant_df.select('merchant_id', 'merchant_name', 'category', 'country', 'risk_level', 'date', 'month', 'year')\n",
    "\n",
    "\n",
    "merchant_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14420c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------------+--------+----------+---------+----+--------+--------------------+\n",
      "|account_id|user_id|current_balance|currency|      date|    month|year|    time|        last_updated|\n",
      "+----------+-------+---------------+--------+----------+---------+----+--------+--------------------+\n",
      "|     A0001|  U0001|       93336.67|     INR|2025-06-09|     June|2025|06:39:38|2025-06-09T06:39:...|\n",
      "|     A0002|  U0002|      171217.88|     INR|2025-07-26|     July|2025|10:07:27|2025-07-26T10:07:...|\n",
      "|     A0003|  U0003|      198817.06|     INR|2025-03-18|    March|2025|20:57:35|2025-03-18T20:57:...|\n",
      "|     A0004|  U0004|      134945.77|     INR|2025-01-30|  January|2025|22:42:45|2025-01-30T22:42:...|\n",
      "|     A0005|  U0005|      133217.58|     INR|2025-08-22|   August|2025|09:30:15|2025-08-22T09:30:...|\n",
      "|     A0006|  U0006|      117262.16|     INR|2025-09-20|September|2025|10:20:54|2025-09-20T10:20:...|\n",
      "|     A0007|  U0007|      154211.77|     INR|2025-04-03|    April|2025|07:02:39|2025-04-03T07:02:...|\n",
      "|     A0008|  U0008|      158804.48|     INR|2025-08-20|   August|2025|06:30:05|2025-08-20T06:30:...|\n",
      "|     A0009|  U0009|      112144.33|     INR|2025-01-04|  January|2025|21:42:44|2025-01-04T21:42:...|\n",
      "|     A0010|  U0010|       38632.98|     INR|2025-02-20| February|2025|10:36:25|2025-02-20T10:36:...|\n",
      "+----------+-------+---------------+--------+----------+---------+----+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#account id starts with A and user id starts with U\n",
    "account_df = account_df.filter(col(\"account_id\").startswith(\"A\") & col(\"user_id\").startswith(\"U\")) \n",
    "                       \n",
    "#extracting date month year from last_updated column\n",
    "account_df = account_df.withColumn(\"date\", to_date(col(\"last_updated\"), \"yyyy-MM-dd\"))\\\n",
    "                    .withColumn(\"month\", date_format(\"date\", \"MMMM\")) \\\n",
    "                    .withColumn(\"year\", date_format(\"date\", \"yyyy\"))\\\n",
    "                    .withColumn(\"Time\",date_format(col(\"last_updated\"), \"HH:mm:ss\"))\n",
    "\n",
    "# #ensuring the userid , accountid , cuurency are Uppercase\n",
    "\n",
    "account_df = account_df.withColumn(\"account_id\", upper(col(\"account_id\"))) \\\n",
    "                       .withColumn(\"user_id\", upper(col(\"user_id\"))) \\\n",
    "                       .withColumn(\"currency\", upper(col(\"currency\")))\n",
    "##duplicate datas\n",
    "duplicate_keys = (\n",
    "    account_df.groupBy(\"account_id\")\n",
    "    .count()\n",
    "    .filter(\"count > 1\")\n",
    ")\n",
    "account_df = account_df.dropDuplicates([\"account_id\"])\n",
    "\n",
    "#filtering the account data for current balance greater than 0 and not null currency\n",
    "\n",
    "account_df = account_df.filter((col(\"current_balance\") > 0) & (col(\"currency\").isNotNull()))\n",
    "\n",
    "account_df.select('account_id', 'user_id', 'current_balance', 'currency', 'date', 'month', 'year','time','last_updated').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d54b1892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "profile_df.write.format(\"parquet\").mode(\"append\").save(f\"s3a://{BUCKET_NAME}/silver/processed-profiles/\")\n",
    "merchant_df.write.format(\"parquet\").mode(\"append\").save(f\"s3a://{BUCKET_NAME}/silver/processed-merchants/\")\n",
    "account_df.write.format(\"parquet\").mode(\"append\").save(f\"s3a://{BUCKET_NAME}/silver/processed-accounts/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
